{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rsglick/drl/blob/master/notebooks/LLCv2_stable_baselines3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle installs for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !apt-get install -y xvfb x11-utils python-opengl swig cmake ffmpeg freeglut3-dev\n",
    "    !pip install Box2D box2d-py box2d-kengz gym[box2d] gym[Box_2D]\n",
    "    !pip install pyvirtualdisplay PyOpenGL piglet piglet-templates PyOpenGL-accelerate\n",
    "    !pip install stable-baselines3[extra]\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib inline\n",
    "#     %matplotlib widget\n",
    "#     %matplotlib notebook\n",
    "#     %matplotlib notebook\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ-2FEM2GwnK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Stable Baselines3 \n",
    "#  https://github.com/DLR-RM/stable-baselines3\n",
    "from stable_baselines3 import PPO, SAC, TD3, A2C\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import CallbackList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LrLvWX9ICxi"
   },
   "source": [
    "# Create Callbacks for monitoring training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir, check_freq=1000, verbose=1, moving_average_window=100):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir   \n",
    "        self.moving_average_window = moving_average_window\n",
    "        \n",
    "    def _on_training_start(self):\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)      \n",
    "        \n",
    "    def _on_step(self):\n",
    "        if self.num_timesteps % self.check_freq == 0:\n",
    "            # Retrieve training reward\n",
    "            x, y = results_plotter.ts2xy(results_plotter.load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                self.writer.add_scalar(\"charts/episode_reward\", y[-1], self.num_timesteps)\n",
    "                self.writer.add_scalar(\"charts/reward_avg\", np.mean(y) , self.num_timesteps )\n",
    "            if len(x) > self.moving_average_window:\n",
    "                moving_average = np.mean(y[-self.moving_average_window:])\n",
    "                self.writer.add_scalar(\"charts/reward_moving_avg\", moving_average, self.num_timesteps )\n",
    "                \n",
    "    def _on_training_end(self):\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        self.writer.close()\n",
    "\n",
    "        \n",
    "class PlottingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for plotting the performance in realtime.\n",
    "\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=1):\n",
    "        super(PlottingCallback, self).__init__(verbose)\n",
    "        self._plot = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # get the monitor's data\n",
    "        x, y = results_plotter.ts2xy(results_plotter.load_results(log_dir), 'timesteps')\n",
    "        if self._plot is None: # make the plot\n",
    "            plt.ion()\n",
    "            fig = plt.figure(figsize=(8,4))\n",
    "            ax = fig.add_subplot(111)\n",
    "            line, = ax.plot(x, y)\n",
    "            self._plot = (line, ax, fig)\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "        else: # update and rescale the plot\n",
    "            self._plot[0].set_data(x, y)\n",
    "            self._plot[-2].relim()\n",
    "            self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02, \n",
    "                                   self.locals[\"total_timesteps\"] * 1.02])\n",
    "            self._plot[-2].autoscale_view(True,True,True)\n",
    "            self._plot[-1].canvas.draw()\n",
    "            \n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param pbar: (tqdm.pbar) Progress bar object\n",
    "    \"\"\"\n",
    "    def __init__(self, pbar):\n",
    "        super(ProgressBarCallback, self).__init__()\n",
    "        self._pbar = pbar\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Update the progress bar:\n",
    "        self._pbar.n = self.num_timesteps\n",
    "        self._pbar.update(0)\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class ProgressBarManager(object):\n",
    "    def __init__(self, total_timesteps): # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "        \n",
    "    def __enter__(self): # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "            \n",
    "        return ProgressBarCallback(self.pbar)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb): # close the callback\n",
    "        self.pbar.n = self.total_timesteps\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZCEdkl5Gvs3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Environment Selected.\n",
      "MountainCarContinuous-v0: reward_threshold 90.0\n"
     ]
    }
   ],
   "source": [
    "env_list = [\n",
    "    \"LunarLanderContinuous-v2\",\n",
    "    \"MountainCarContinuous-v0\",    \n",
    "    \"Pendulum-v0\",    \n",
    "    \"BipedalWalker-v3\",\n",
    "    \"BipedalWalkerHardcore-v3\",   \n",
    "    \"CarRacing-v0\",\n",
    "    # Mujoco Envs\n",
    "    \"Ant-v3\",\n",
    "    \"Walker2d-v3\",\n",
    "    \"HalfCheetah-v3\",\n",
    "    \"Humanoid-v3\",\n",
    "    \"InvertedPendulum-v2\",\n",
    "    \"InvertedDoublePendulum-v2\",\n",
    "    \"HumanoidStandup-v2\",\n",
    "]\n",
    "\n",
    "# all_envs = [i for i in gym.envs.registry.all()]\n",
    "# [gym.make(env) for env in env_list]\n",
    "# [gym.make(env).action_space for env in env_list]\n",
    "# [gym.make(env).observation_space for env in env_list]\n",
    "\n",
    "# [gym.envs.registry.env_specs[env_name].max_episode_steps for env in env_list]\n",
    "# [gym.envs.registry.env_specs[env].reward_threshold for env in env_list ]\n",
    "\n",
    "env_name = env_list[1]\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "if type(env.action_space) == gym.spaces.box.Box:\n",
    "    print(\"Continuous Environment Selected.\")\n",
    "else:\n",
    "    print(\"This is not a continuous environment\")\n",
    "\n",
    "cur_env_specs = gym.envs.registry.env_specs[env_name]\n",
    "reward_threshold = gym.envs.registry.env_specs[env_name].reward_threshold\n",
    "\n",
    "total_timesteps = 300000\n",
    "callbackFreq = 1000\n",
    "\n",
    "log_dir = f\"./runs/{env_name}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "\n",
    "eval_env = gym.make(env_name)\n",
    "\n",
    "action_space_dim = env.action_space.shape[0]\n",
    "observation_space_dim = env.observation_space.shape[0]\n",
    "feature_dim = action_space_dim + observation_space_dim\n",
    "print(f\"{env_name}: reward_threshold {reward_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensorboard_callback =TensorboardCallback(log_dir=log_dir, \n",
    "                                          check_freq=callbackFreq)\n",
    "plotting_callback = PlottingCallback()\n",
    "reward_threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=reward_threshold, \n",
    "                                                          verbose=1)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=callbackFreq, \n",
    "                                         save_path=log_dir)\n",
    "eval_callback = EvalCallback(eval_env, \n",
    "                             render=False,\n",
    "                             best_model_save_path=log_dir,\n",
    "                             deterministic=False,\n",
    "                             callback_on_new_best=reward_threshold_callback,\n",
    "                             log_path=log_dir, \n",
    "                             verbose=1,\n",
    "                             n_eval_episodes=10,\n",
    "                             eval_freq=callbackFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "Set up Tensorboard to track traing visually. \n",
    "\n",
    "#TODO Provide more details in Tensorflow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d46a737ccc5a52eb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d46a737ccc5a52eb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhetAU8EHHsr"
   },
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Wm63P8AcMIgi",
    "outputId": "9dbc17cc-f97e-4cd2-edf3-2c90ce9c44fd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparams derived from the SAC paper\n",
    "hidden_dims = 256\n",
    "sac_hyperparams = {\n",
    "    'learning_rate': 3.0e-4,\n",
    "    'buffer_size': 1000000,\n",
    "    'gamma': 0.99,\n",
    "    'batch_size':256,\n",
    "    'tau': 0.005,\n",
    "    'device':'cuda',\n",
    "    'seed':0,\n",
    "    'target_entropy':\"auto\",\n",
    "    'policy_kwargs':dict(net_arch=[hidden_dims, hidden_dims]),\n",
    "}\n",
    "\n",
    "# # https://github.com/openai/gym/wiki/Leaderboard#lunarlandercontinuous-v2\n",
    "# hidden_dims = 64\n",
    "# sac_hyperparams = {\n",
    "#     'learning_rate': 5.0e-3,\n",
    "#     'buffer_size': 200000,\n",
    "#     'gamma': 0.999,\n",
    "#     'batch_size':8192,\n",
    "#     'tau': 0.005,\n",
    "#     'device':'cuda',\n",
    "#     'seed':0,\n",
    "#     'target_entropy':\"auto\",\n",
    "#     'policy_kwargs':dict(net_arch=[hidden_dims, hidden_dims]),\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelSAC = SAC('MlpPolicy', \n",
    "               env,\n",
    "               #verbose=1,\n",
    "               **sac_hyperparams)\n",
    "\n",
    "if os.path.exists(f\"{log_dir}/modelSAC_{env_name}.zip\"):\n",
    "    print(\"Loading existing Model...\")\n",
    "    modelSAC = SAC.load(f\"{log_dir}/modelSAC_{env_name}\",\n",
    "                       env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tensorboard Graphs for some visualizations \n",
    "\n",
    "try:\n",
    "    with SummaryWriter(f\"{log_dir}/graphs/CriticQ1\") as writer:\n",
    "        dummy_input = (torch.zeros(1, feature_dim))\n",
    "        writer.add_graph(modelSAC.policy.critic.q1_net, input_to_model=dummy_input )\n",
    "\n",
    "    with SummaryWriter(f\"{log_dir}/graphs/CriticQ2\") as writer:\n",
    "        dummy_input = (torch.zeros(1, feature_dim))\n",
    "        writer.add_graph(modelSAC.policy.critic.q2_net, input_to_model=dummy_input )\n",
    "\n",
    "\n",
    "    with SummaryWriter(f\"{log_dir}/graphs/Actor_pi\") as writer:\n",
    "        dummy_input = (torch.zeros(1, observation_space_dim))\n",
    "        writer.add_graph(modelSAC.policy.actor.latent_pi, input_to_model=dummy_input )\n",
    "\n",
    "    with SummaryWriter(f\"{log_dir}/graphs/Actor_mu\") as writer:\n",
    "        dummy_input = (torch.zeros(1, hidden_dims))\n",
    "        writer.add_graph(modelSAC.policy.actor.mu, input_to_model=dummy_input )\n",
    "\n",
    "    with SummaryWriter(f\"{log_dir}/graphs/Actor_log_std\") as writer:\n",
    "        dummy_input = (torch.zeros(1, hidden_dims))\n",
    "        writer.add_graph(modelSAC.policy.actor.log_std, input_to_model=dummy_input )\n",
    "except Exception as inst:\n",
    "    print(f\"ERROR: {inst}\")\n",
    "    print(\"Skipping Graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SqzCqsHyMLiw",
    "outputId": "bdcabc3b-8b1e-4b89-bd94-df37690106c4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cca829d3bc5422a91ecea0e353609d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-35.11 +/- 0.78\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-32.61 +/- 0.91\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-31.54 +/- 0.91\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-32.45 +/- 1.41\n",
      "Episode length: 999.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-30.66 +/- 1.05\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-29.31 +/- 0.63\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-28.00 +/- 0.83\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-26.11 +/- 0.66\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-22.44 +/- 0.72\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-21.20 +/- 0.53\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-15.56 +/- 0.37\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-11.50 +/- 0.48\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-12.14 +/- 0.40\n",
      "Episode length: 999.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-7.10 +/- 0.36\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-5.54 +/- 0.14\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-4.90 +/- 0.20\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-3.61 +/- 0.16\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-3.78 +/- 0.21\n",
      "Episode length: 999.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-3.58 +/- 0.38\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.25 +/- 0.12\n",
      "Episode length: 999.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=-3.05 +/- 0.14\n",
      "Episode length: 999.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-2.46 +/- 0.05\n",
      "Episode length: 999.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2.27 +/- 0.10\n",
      "Episode length: 999.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ProgressBarManager(total_timesteps) as progress_callback:\n",
    "    callback = CallbackList([progress_callback,\n",
    "                             #checkpoint_callback,\n",
    "                             #plotting_callback,\n",
    "                             tensorboard_callback,\n",
    "                             eval_callback])\n",
    "    modelSAC.learn(total_timesteps=total_timesteps, \n",
    "                   callback=callback,\n",
    "                   )\n",
    "\n",
    "\n",
    "# Save the agent\n",
    "modelSAC.save(f\"{log_dir}/modelSAC_{env_name}\")\n",
    "del modelSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "ln-5H8q0G0Fd",
    "outputId": "f89d77d4-69dd-4f25-d2fc-ae18290ecebc"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluate the trained agent\n",
    "modelSAC = SAC.load(f\"{log_dir}/modelSAC_{env_name}\")\n",
    "\n",
    "\n",
    "if os.stat(f\"{log_dir}/monitor.csv\").st_size > 100:\n",
    "    results_plotter.plot_results(dirs=[log_dir], \n",
    "                                 num_timesteps=None,\n",
    "                                 x_axis=results_plotter.X_TIMESTEPS, \n",
    "                                 task_name=f\"modelSAC_{env_name}\",\n",
    "                                 figsize=(8,4)\n",
    "                                )\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(modelSAC, \n",
    "                                          eval_env, \n",
    "                                          render=False,\n",
    "                                          n_eval_episodes=100)\n",
    "eval_env.close()\n",
    "print(f\"mean_reward {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "def test_agent(model, \n",
    "               render: bool = False,\n",
    "               timesteps: int = 1000000, \n",
    "               max_eps: int = 100,\n",
    "               env_name: str = \"LunarLanderContinuous-v2\"):\n",
    "    try:\n",
    "        eval_env = gym.make(env_name)\n",
    "\n",
    "        episode_rewards, episode_lengths = [], [] \n",
    "        episode_reward = 0.0\n",
    "        episode_length = 0\n",
    "        num_episodes   = 0\n",
    "        obs = eval_env.reset()\n",
    "        pbar = tqdm(total=timesteps)\n",
    "        pbar_eps = tqdm(total=max_eps)\n",
    "        #for i in tqdm(range(timesteps)):\n",
    "        for i in range(timesteps):\n",
    "            pbar.update()\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, rewards, dones, info = eval_env.step(action)\n",
    "            episode_reward += rewards\n",
    "            episode_length += 1\n",
    "            \n",
    "            if render:\n",
    "                eval_env.render()\n",
    "            if dones:\n",
    "                pbar_eps.update()\n",
    "                num_episodes += 1\n",
    "                pbar.write(f\"Episode({num_episodes}) episode_reward = {episode_reward:.2f}, episode_length = {episode_length:.2f}\")\n",
    "\n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_lengths.append(episode_length)\n",
    "\n",
    "                episode_reward = 0.0\n",
    "                episode_length = 0\n",
    "                eval_env.reset()\n",
    "                \n",
    "                if num_episodes == max_eps:\n",
    "                    break\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                pbar.write(f\"Step {i}\")\n",
    "        pbar.refresh()\n",
    "        pbar.close()\n",
    "        pbar_eps.refresh()\n",
    "        pbar_eps.close()\n",
    "        eval_env.close()\n",
    "\n",
    "        mean_reward = np.mean(episode_rewards)\n",
    "        std_reward = np.std(episode_rewards)\n",
    "\n",
    "        print(f\"Total Eps({num_episodes}): mean_reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pbar.refresh()\n",
    "        pbar.close()\n",
    "        pbar_eps.refresh()\n",
    "        pbar_eps.close()\n",
    "        eval_env.close()\n",
    "        return\n",
    "\n",
    "test_agent(modelSAC,\n",
    "           max_eps=10,\n",
    "           env_name=env_name,\n",
    "           render=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aoZ0KicdHP67"
   },
   "source": [
    "# Record agents in action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-38PJcHlHPWM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_videos(video_path='', prefix=''):\n",
    "  \"\"\"\n",
    "  Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "  :param video_path: (str) Path to the folder containing videos\n",
    "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "  \"\"\"\n",
    "  html = []\n",
    "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "      html.append('''<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "  \"\"\"\n",
    "  :param env_id: (str)\n",
    "  :param model: (RL model)\n",
    "  :param video_length: (int)\n",
    "  :param prefix: (str)\n",
    "  :param video_folder: (str)\n",
    "  \"\"\"\n",
    "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "  # Start the video at step=0 and record 500 steps\n",
    "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "  obs = eval_env.reset()\n",
    "  for _ in range(video_length):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "  # Close the video recorder\n",
    "  eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "q-iHFGyKHYSh",
    "outputId": "11dce7a6-7fce-4712-9713-bf75097f9bc1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#video_folder = f\"{env_name}_output\"\n",
    "video_folder = log_dir\n",
    "\n",
    "# record_video(env_name, modelTD3, video_length=1000, prefix=f'td3_{env_name}', video_folder=video_folder)\n",
    "# record_video(env_name, modelPPO, video_length=1000, prefix=f'ppo_{env_name}', video_folder=video_folder)\n",
    "record_video(env_name, modelSAC, video_length=2000, prefix=f'sac_{env_name}', video_folder=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "owcltuOZHe4w",
    "outputId": "a6a9719a-d166-47f0-ab43-8116f3308a9f"
   },
   "outputs": [],
   "source": [
    "show_videos(log_dir, prefix=f'sac_{env_name}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNbO8bFY5jsMgnEIKw5oI7L",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "LLCv2_stable_baselines3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
