{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "continousGymEnvAlgs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaM9X1bbGdWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "\n",
        "apt-get install -y xvfb x11-utils python-opengl swig cmake ffmpeg freeglut3-dev\n",
        "\n",
        "#pip install ray ray[rllib] ray[debug]\n",
        "pip install Box2D box2d-py box2d-kengz gym[box2d] gym[Box_2D]\\\n",
        "            pyvirtualdisplay\\\n",
        "            PyOpenGL\\\n",
        "            piglet\\\n",
        "            piglet-templates\\\n",
        "            PyOpenGL-accelerate\\\n",
        "            stable-baselines3[extra]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBiSvhyoGpWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ-2FEM2GwnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from stable_baselines3 import PPO, SAC, TD3, A2C\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LrLvWX9ICxi",
        "colab_type": "text"
      },
      "source": [
        "# Create Continous Gym Environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZCEdkl5Gvs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LunarLanderContinuous-v2\n",
        "# MountainCarContinuous-v0\n",
        "# Pendulum-v0\n",
        "\n",
        "env_name = \"LunarLanderContinuous-v2\"\n",
        "env = gym.make(env_name)\n",
        "\n",
        "log_dir = f\"./gym/{env_name}\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "env = Monitor(env, log_dir)\n",
        "callback = EvalCallback(env, log_path=log_dir, eval_freq=1000, deterministic=False )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7KsW0bTIknr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 1000 #300000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhetAU8EHHsr",
        "colab_type": "text"
      },
      "source": [
        "# SAC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln-5H8q0G0Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "sac_hyperparams = {\n",
        "    'learning_rate': 3.0e-4,\n",
        "    'buffer_size': 1000000,\n",
        "    'gamma': 0.99,\n",
        "    'batch_size':256,\n",
        "    'tau': 0.005,\n",
        "    'device':'cuda',\n",
        "}\n",
        "\n",
        "\n",
        "modelSAC = SAC('MlpPolicy', env, **sac_hyperparams)\n",
        "modelSAC.learn(total_timesteps=total_timesteps, callback=callback)\n",
        "\n",
        "# Save the agent\n",
        "\n",
        "modelSAC.save(f\"modelSAC_{env_name}\")\n",
        "del modelSAC\n",
        "\n",
        "# Evaluate the trained agent\n",
        "modelSAC = SAC.load(f\"modelSAC_{env_name}\")\n",
        "\n",
        "eval_env = gym.make(env_name)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(modelSAC, eval_env, n_eval_episodes=100, deterministic=False)\n",
        "\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKz1FLz7HFBC",
        "colab_type": "text"
      },
      "source": [
        "# PPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38h1LtFNG0-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "modelPPO = PPO('MlpPolicy', env)\n",
        "modelPPO.learn(total_timesteps=total_timesteps, callback=callback)\n",
        "\n",
        "modelPPO.save(f\"modelPPO_{env_name}\")\n",
        "del modelPPO\n",
        "\n",
        "# Evaluate the trained agent\n",
        "modelPPO = PPO.load(f\"modelPPO_{env_name}\")\n",
        "\n",
        "eval_env = gym.make(env_name)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(modelPPO, eval_env, n_eval_episodes=100, deterministic=False)\n",
        "\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn3hH0c0HKCP",
        "colab_type": "text"
      },
      "source": [
        "# TD3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JfKVAhcG9mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "modelTD3 = TD3('MlpPolicy', env)\n",
        "modelTD3.learn(total_timesteps=total_timesteps, callback=callback)\n",
        "\n",
        "modelTD3.save(f\"modelTD3_{env_name}\")\n",
        "del modelTD3\n",
        "\n",
        "\n",
        "# Evaluate the trained agent\n",
        "modelTD3 = TD3.load(f\"modelTD3_{env_name}\")\n",
        "\n",
        "eval_env = gym.make(env_name)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(modelTD3, eval_env, n_eval_episodes=100, deterministic=False)\n",
        "\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoZ0KicdHP67",
        "colab_type": "text"
      },
      "source": [
        "# Record agents in action\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-38PJcHlHPWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-iHFGyKHYSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "record_video(env_name, modelTD3, video_length=1000, prefix=f'td3_{env_name}')\n",
        "record_video(env_name, modelPPO, video_length=1000, prefix=f'ppo_{env_name}')\n",
        "record_video(env_name, modelSAC, video_length=1000, prefix=f'sac_{env_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owcltuOZHe4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_videos('videos', prefix=f'sac_{env_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V7uxa-WHhls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_videos('videos', prefix=f'td3_{env_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNJAZmmJHgZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_videos('videos', prefix=f'ppo_{env_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGSXQ0A8HlyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(f'/content/td3_{env_name}.zip')\n",
        "files.download(f'/content/ppo_{env_name}.zip')\n",
        "files.download(f'/content/sac_{env_name}.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}